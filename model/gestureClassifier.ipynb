{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose Extractor\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # 얼굴 랜드마크 그리기\n",
    "    # mp_drawing.draw_landmarks(\n",
    "    #     image, results.face_landmarks, mp.solutions.face_mesh.FACEMESH_TESSELATION, \n",
    "    #     mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "    #     mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    # )\n",
    "    # 포즈 랜드마크 그리기\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "        mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    \n",
    "    # 왼팔 랜드마크 그리기\n",
    "    if results.pose_landmarks:\n",
    "        left_arm = [\n",
    "            mp_holistic.PoseLandmark.LEFT_SHOULDER,\n",
    "            mp_holistic.PoseLandmark.LEFT_ELBOW,\n",
    "            mp_holistic.PoseLandmark.LEFT_WRIST\n",
    "        ]\n",
    "        for landmark in left_arm:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmark]\n",
    "            landmark_coords = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "            cv2.circle(image, landmark_coords, 5, (255,0,0), -1)  # 파란색으로 표시\n",
    "\n",
    "    # 오른팔 랜드마크 그리기\n",
    "    if results.pose_landmarks:\n",
    "        right_arm = [\n",
    "            mp_holistic.PoseLandmark.RIGHT_SHOULDER,\n",
    "            mp_holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "            mp_holistic.PoseLandmark.RIGHT_WRIST\n",
    "        ]\n",
    "        for landmark in right_arm:\n",
    "            landmark_point = results.pose_landmarks.landmark[landmark]\n",
    "            landmark_coords = (int(landmark_point.x * image.shape[1]), int(landmark_point.y * image.shape[0]))\n",
    "            cv2.circle(image, landmark_coords, 5, (0,0,255), -1)  # 빨간색으로 표시\n",
    "\n",
    "    # 왼손 랜드마크 그리기\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    # 오른손 랜드마크 그리기\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "        mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "        mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        # cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "\t # 포즈, 얼굴, 손 랜드마크 추출\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    # 왼팔 랜드마크 추출\n",
    "    left_arm = np.array([[results.pose_landmarks.landmark[landmark].x,\n",
    "                          results.pose_landmarks.landmark[landmark].y,\n",
    "                          results.pose_landmarks.landmark[landmark].z,\n",
    "                          results.pose_landmarks.landmark[landmark].visibility]\n",
    "                         for landmark in [mp_holistic.PoseLandmark.LEFT_SHOULDER,\n",
    "                                          mp_holistic.PoseLandmark.LEFT_ELBOW,\n",
    "                                          mp_holistic.PoseLandmark.LEFT_WRIST]]).flatten() if results.pose_landmarks else np.zeros(12)\n",
    "\n",
    "    # 오른팔 랜드마크 추출\n",
    "    right_arm = np.array([[results.pose_landmarks.landmark[landmark].x,\n",
    "                           results.pose_landmarks.landmark[landmark].y,\n",
    "                           results.pose_landmarks.landmark[landmark].z,\n",
    "                           results.pose_landmarks.landmark[landmark].visibility]\n",
    "                          for landmark in [mp_holistic.PoseLandmark.RIGHT_SHOULDER,\n",
    "                                           mp_holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "                                           mp_holistic.PoseLandmark.RIGHT_WRIST]]).flatten() if results.pose_landmarks else np.zeros(12)\n",
    "\n",
    "    # 최종 배열에 모든 랜드마크를 포함시킴\n",
    "    final_landmarks = np.concatenate([pose, lh, rh, left_arm, right_arm])\n",
    "\n",
    "    return final_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /home/seongmin/documents/A2I_Project/model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 현재 작업 디렉토리 확인\n",
    "current_directory = os.getcwd()\n",
    "print(\"현재 작업 디렉토리:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IllegalBlock\n",
      "IllegalStart\n",
      "PointRight\n",
      "BeginServeLeft\n",
      "PointLeft\n",
      "BeginServeRight\n",
      "IllegalBackRowAttack\n",
      "TimeOutRight\n",
      "IllegalHit\n",
      "BallTouched\n",
      "FourHits\n",
      "LineViolation\n",
      "Replay\n",
      "OverNet\n",
      "DoubleHit\n",
      "TimeOutLeft\n",
      "BallOut\n",
      "BallIn\n",
      "CourtChange\n",
      "DelayofService\n",
      "Substitution\n",
      "NetFoul\n",
      "EndofSet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 경로 설정 (위의 방법 중 하나를 사용)\n",
    "directory = r'/home/seongmin/documents/A2I_Project/data/volleyball signals'\n",
    "\n",
    "dir_name = []\n",
    "\n",
    "# 디렉토리 내의 모든 폴더 이름 가져오기\n",
    "for folder_name in os.listdir(directory):\n",
    "    full_path = os.path.join(directory, folder_name)\n",
    "    if os.path.isdir(full_path):\n",
    "        dir_name.append(folder_name)\n",
    "        print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('../data/volleyball signals') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array([\n",
    "    \"IllegalBlock\",\n",
    "    \"IllegalStart\",\n",
    "    \"PointRight\",\n",
    "    \"BeginServeLeft\",\n",
    "    \"PointLeft\",\n",
    "    \"BeginServeRight\",\n",
    "    \"IllegalBackRowAttack\",\n",
    "    \"TimeOutRight\",\n",
    "    \"IllegalHit\",\n",
    "    \"BallTouched\",\n",
    "    \"FourHits\",\n",
    "    \"LineViolation\",\n",
    "    \"Replay\",\n",
    "    \"OverNet\",\n",
    "    \"DoubleHit\",\n",
    "    \"TimeOutLeft\",\n",
    "    \"BallOut\",\n",
    "    \"BallIn\",\n",
    "    \"CourtChange\",\n",
    "    \"DelayofService\",\n",
    "    \"Substitution\",\n",
    "    \"NetFoul\",\n",
    "    \"EndofSet\"\n",
    "])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30 \n",
    "\n",
    "# Path for tfrecords\n",
    "TFRECORD_PATH = '../data/tfrecord/features.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):  # if value is a tensor\n",
    "        value = value.numpy()  # get the value of the tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(image, keypoints, label):\n",
    "    feature = {\n",
    "        'image': _bytes_feature(image),\n",
    "        'keypoints': tf.train.Feature(float_list=tf.train.FloatList(value=keypoints)),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(video_data, labels, output_path):\n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        for idx, frames in enumerate(video_data):\n",
    "            action = labels[idx]\n",
    "            for frame in frames:\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                keypoints = extract_keypoints(results)\n",
    "\n",
    "                # Create TFRecord example\n",
    "                tf_example = create_tf_example(frame, keypoints, action)\n",
    "                writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_with_optical_flow(video_path, target_frame_count):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < target_frame_count:\n",
    "        while len(frames) < target_frame_count:\n",
    "            new_frames = []\n",
    "            for i in range(len(frames) - 1):\n",
    "                new_frames.append(frames[i])\n",
    "                interpolated_frame = interpolate_frames(frames[i], frames[i + 1], 0.5)\n",
    "                new_frames.append(interpolated_frame)\n",
    "            new_frames.append(frames[-1])\n",
    "            frames = new_frames[:target_frame_count]\n",
    "\n",
    "    elif len(frames) > target_frame_count:\n",
    "        indices = np.linspace(0, len(frames) - 1, target_frame_count).astype(int)\n",
    "        frames = [frames[i] for i in indices]\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def interpolate_frames(frame1, frame2, interpolation_ratio):\n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    h, w = frame1.shape[:2]\n",
    "    flow_map = np.column_stack(np.mgrid[0:h, 0:w]).astype(np.float32)\n",
    "    flow_map += flow * interpolation_ratio\n",
    "    \n",
    "    interpolated_frame = cv2.remap(frame1, flow_map, None, cv2.INTER_LINEAR)\n",
    "    return interpolated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # 색상 변환\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)  # 처리\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp.solutions.drawing_utils.draw_landmarks(\n",
    "        image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() \\\n",
    "        if results.pose_landmarks else np.zeros(33 * 4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos_from_directory(directory, target_frame_count=30):\n",
    "    video_data = []\n",
    "    labels = []\n",
    "    for label in os.listdir(directory):\n",
    "        label_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for video_file in os.listdir(label_dir):\n",
    "                video_path = os.path.join(label_dir, video_file)\n",
    "                frames = process_video_with_optical_flow(video_path, target_frame_count)\n",
    "                video_data.append(frames)\n",
    "                labels.append(label)\n",
    "    return video_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    video_data, labels = load_videos_from_directory(DATA_PATH, target_frame_count=sequence_length)\n",
    "    write_tfrecord(video_data, labels, TFRECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IllegalBlock': 0,\n",
       " 'IllegalStart': 1,\n",
       " 'PointRight': 2,\n",
       " 'BeginServeLeft': 3,\n",
       " 'PointLeft': 4,\n",
       " 'BeginServeRight': 5,\n",
       " 'IllegalBackRowAttack': 6,\n",
       " 'TimeOutRight': 7,\n",
       " 'IllegalHit': 8,\n",
       " 'BallTouched': 9,\n",
       " 'FourHits': 10,\n",
       " 'LineViolation': 11,\n",
       " 'Replay': 12,\n",
       " 'OverNet': 13,\n",
       " 'DoubleHit': 14,\n",
       " 'TimeOutLeft': 15,\n",
       " 'BallOut': 16,\n",
       " 'BallIn': 17,\n",
       " 'CourtChange': 18,\n",
       " 'DelayofService': 19,\n",
       " 'Substitution': 20,\n",
       " 'NetFoul': 21,\n",
       " 'EndofSet': 22}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
